{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 3, 20]), torch.Size([50, 3, 20]))"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SelfRNN(nn.Module):\n",
    "    def __init__(self,in_dim,hidden_dim,num_layers):\n",
    "        super().__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.ModuleList()\n",
    "        for _ in range(self.num_layers):\n",
    "            self.rnn.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(in_dim + hidden_dim,hidden_dim),\n",
    "                    nn.ReLU(),\n",
    "                ))\n",
    "            in_dim = hidden_dim\n",
    "    def forward(self,in_features,h0 = None):\n",
    "        seq_len,batch_size,_ = in_features.shape\n",
    "        if h0 == None:\n",
    "            h0 = torch.zeros(batch_size,self.hidden_dim)\n",
    "        output = []\n",
    "        for time_step in range(seq_len):\n",
    "            x = in_features[time_step,:,:]\n",
    "            hn = []\n",
    "            for i in range(self.num_layers):\n",
    "                hi = h0[i]\n",
    "                x = self.rnn[i](torch.cat((x,hi),dim = 1))\n",
    "                hn.append(x)\n",
    "            h0 = torch.stack(hn)\n",
    "            output.append(x)\n",
    "        out = torch.stack(output)\n",
    "        return out,h0\n",
    "rnn = SelfRNN(10, 20, 50)\n",
    "input = torch.randn(5, 3, 10)\n",
    "h0 = torch.randn(50, 3, 20)\n",
    "output,hn = rnn(input, h0)\n",
    "output.shape, hn.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 20]) torch.Size([2, 3, 20]) torch.Size([5, 3, 20])\n"
     ]
    }
   ],
   "source": [
    "class SelfLSTM(nn.Module):\n",
    "    def __init__(self,in_dim,embedding_dim,num_layers):\n",
    "        super().__init__()\n",
    "        self.in_dim = in_dim \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.ModuleList()\n",
    "        for _ in range(self.num_layers):\n",
    "            self.lstm.append(nn.Linear(in_dim + embedding_dim,embedding_dim * 4))\n",
    "            in_dim = embedding_dim\n",
    "    def forward(self,in_features,h0 = None,c0 = None):\n",
    "        seq_len,batch_size,_ = in_features.shape\n",
    "        if h0 == None:\n",
    "            h0 = torch.zeros(seq_len,batch_size,self.embedding_dim)\n",
    "        if c0 == None:\n",
    "            c0 = torch.zeros(seq_len,batch_size,self.embedding_dim)\n",
    "        output = []\n",
    "        for time_step in range(seq_len):\n",
    "            x = in_features[time_step].squeeze(0)\n",
    "            hn = []\n",
    "            cn = []\n",
    "            for i in range(self.num_layers):\n",
    "                hi = h0[i]\n",
    "                ci = c0[i]\n",
    "                x = self.lstm[i](torch.cat((x,hi),dim = 1))\n",
    "                i_t,f_t,g_t,o_t, = x.split(self.embedding_dim,dim = -1)\n",
    "                i_t = torch.sigmoid(i_t)\n",
    "                f_t = torch.sigmoid(f_t)\n",
    "                g_t = torch.tanh(g_t)\n",
    "                o_t = torch.sigmoid(o_t)\n",
    "                ci = g_t * i_t + f_t * ci\n",
    "                x = ci * o_t\n",
    "                hn.append(x)\n",
    "                cn.append(ci)\n",
    "            h0 = torch.stack(hn)\n",
    "            output.append(x)\n",
    "        out = torch.stack(output)\n",
    "        return out,h0,c0\n",
    "\n",
    "        \n",
    "lstm = SelfLSTM(10, 20, 2)\n",
    "input = torch.randn(5, 3, 10)\n",
    "h0 = torch.randn(2, 3, 20)\n",
    "output, hn,C = lstm(input, h0)\n",
    "print(output.shape,hn.shape,C.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 4, 40])\n"
     ]
    }
   ],
   "source": [
    "class SelfMultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self,embedding_dim,n_head):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.n_head = n_head\n",
    "        self.head_dim = embedding_dim // n_head\n",
    "        assert embedding_dim % n_head == 0\n",
    "        self.attenion = nn.Linear(embedding_dim,embedding_dim*3)\n",
    "        self.f_out = nn.Linear(embedding_dim,embedding_dim)\n",
    "    def forward(self,in_features):\n",
    "        batch_size,seq_len,embedding_dim = in_features.shape\n",
    "        k,q,v = self.attenion(in_features).split(self.embedding_dim,-1)\n",
    "        k = k.view(batch_size,seq_len,self.n_head,self.head_dim).transpose(1,2)\n",
    "        q = q.view(batch_size,seq_len,self.n_head,self.head_dim).transpose(1,2)\n",
    "        v = v.view(batch_size,seq_len,self.n_head,self.head_dim).transpose(1,2)\n",
    "        score =  torch.sigmoid(torch.matmul(q,k.transpose(-1,-2)) / math.sqrt(embedding_dim))\n",
    "        output = score @ v\n",
    "        output = output.transpose(1,2).contiguous().view(batch_size,seq_len,embedding_dim)\n",
    "        output = self.f_out(output)\n",
    "        return output\n",
    "\n",
    "# Line Test\n",
    "\n",
    "net = SelfMultiHeadSelfAttention(40,5)\n",
    "# loss = nn.modules.MSELoss()\n",
    "# optimizer = torch.optim.SGD(net.parameters())\n",
    "in_features = torch.ones(40,4,40)\n",
    "out_feature= net(in_features)\n",
    "print(out_feature.shape)\n",
    "# l = loss(in_features,out_feature)\n",
    "# l.backward()\n",
    "# optimizer.step()\n",
    "# optimizer.zero_grad()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 4, 40])\n"
     ]
    }
   ],
   "source": [
    "class SelfMultiHeadAttention(nn.Module):\n",
    "    def __init__(self,embedding_dim,n_head):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.n_head = n_head\n",
    "        self.head_dim = embedding_dim // n_head\n",
    "        assert embedding_dim % n_head == 0\n",
    "        self.query_linear = nn.Linear(embedding_dim,embedding_dim)\n",
    "        self.key_linear = nn.Linear(embedding_dim,embedding_dim)\n",
    "        self.value_linear = nn.Linear(embedding_dim,embedding_dim)\n",
    "        self.f_out = nn.Linear(embedding_dim,embedding_dim)\n",
    "    def forward(self,q,k,v,mask = None):\n",
    "        # in_features = torch.cat((q,k,v),dim = -1)\n",
    "        batch_size,seq_len,embedding_dim = q.shape\n",
    "        # print(batch_size,seq_len,embedding_dim)\n",
    "        q = self.query_linear(q)\n",
    "        k = self.key_linear(k)\n",
    "        v = self.key_linear(v)\n",
    "        k = k.view(batch_size,seq_len,self.n_head,self.head_dim).transpose(1,2)\n",
    "        q = q.view(batch_size,seq_len,self.n_head,self.head_dim).transpose(1,2)\n",
    "        v = v.view(batch_size,seq_len,self.n_head,self.head_dim).transpose(1,2)\n",
    "        score =  torch.sigmoid(torch.matmul(q,k.transpose(-1,-2)) / math.sqrt(self.embedding_dim))\n",
    "        if mask != None:\n",
    "            score = score.masked_fill(mask == 1,1e-9)\n",
    "        output = score @ v\n",
    "        output = output.transpose(1,2).contiguous().view(batch_size,seq_len,embedding_dim)\n",
    "        output = self.f_out(output)\n",
    "        return output\n",
    "\n",
    "# Line Test\n",
    "\n",
    "net = SelfMultiHeadAttention(40,5)\n",
    "# loss = nn.modules.MSELoss()\n",
    "# optimizer = torch.optim.SGD(net.parameters())\n",
    "in_features = torch.ones(40,4,40)\n",
    "out_feature= net(in_features,in_features,in_features)\n",
    "print(out_feature.shape)\n",
    "# l = loss(in_features,out_feature)\n",
    "# l.backward()\n",
    "# optimizer.step()\n",
    "# optimizer.zero_grad()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfPositionalEncoding(nn.Module):\n",
    "    def __init__(self,embedding_dim,max_length = 5000):\n",
    "        super().__init__()\n",
    "        self.pe = torch.zeros(max_length,embedding_dim)\n",
    "        pos = torch.arange(0,max_length).unsqueeze(1)\n",
    "        div_item = torch.exp(torch.arange(0,embedding_dim,2).float() *(- math.log(1000)) / (embedding_dim) )\n",
    "\n",
    "        self.pe[:,0::2] = torch.sin(pos*div_item)\n",
    "        self.pe[:,1::2] = torch.cos(pos*div_item)\n",
    "    def forward(self,in_features):\n",
    "        batch_size,seq_len,_ = in_features.shape\n",
    "        in_features = in_features + self.pe[0:seq_len]\n",
    "        return in_features\n",
    "    \n",
    "net = SelfPositionalEncoding(500)\n",
    "\n",
    "in_features = torch.rand(4,20,500)\n",
    "\n",
    "out_features = net(in_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfFeedForward(nn.Module):\n",
    "    def __init__(self,d_model,d_ffn):\n",
    "        super().__init__()\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model,d_ffn),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ffn,d_model)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        return self.ffn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfEncoderLayer(nn.Module):\n",
    "    def __init__(self,d_model,d_ff,n_head,dropout):\n",
    "        super().__init__()\n",
    "        self.attention = SelfMultiHeadAttention(d_model,n_head)\n",
    "        self.ffn = SelfFeedForward(d_model,d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self,x):\n",
    "        attention_out = self.attention(x,x,x)\n",
    "        x = self.norm1(x + self.dropout(attention_out))\n",
    "        ffn_out = self.ffn(x)\n",
    "        x = self.norm1(x + self.dropout(ffn_out))\n",
    "        return x\n",
    "\n",
    "in_features = torch.rand(30,5,80)\n",
    "layer = SelfEncoderLayer(80,200,20,0.1)\n",
    "out_features = layer(in_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfDecoderLayer(nn.Module):\n",
    "    def __init__(self,d_model,d_ff,n_head,dropout):\n",
    "        super().__init__()\n",
    "        self.self_attention = SelfMultiHeadAttention(d_model,n_head)\n",
    "        self.cross_attention = SelfMultiHeadAttention(d_model,n_head)\n",
    "        self.ff = SelfFeedForward(d_model,d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "    def forward(self,x,encode_out,src_mask = None,target_mask = None):\n",
    "        attention1_out = self.self_attention(x,x,x,target_mask)\n",
    "        x = self.norm1(x + self.dropout(attention1_out))\n",
    "\n",
    "        attention2_out = self.cross_attention(x,encode_out,encode_out,src_mask)\n",
    "        x = self.norm2(x + self.dropout(attention2_out))\n",
    "\n",
    "        ff_out = self.ff(x)\n",
    "        x = self.norm3(x + self.dropout(ff_out))\n",
    "        return x\n",
    "\n",
    "\n",
    "encoder_out = torch.rand(20,5,80)\n",
    "x = torch.rand(20,5,80)\n",
    "\n",
    "decoder = SelfDecoderLayer(80,100,8,0.1)\n",
    "src_mask = torch.triu(torch.ones(5,5))\n",
    "\n",
    "out = decoder(x,encoder_out,src_mask,src_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0013, 0.0042, 0.0042,  ..., 0.0022, 0.0041, 0.0007],\n",
       "         [0.0007, 0.0034, 0.0030,  ..., 0.0038, 0.0039, 0.0007],\n",
       "         [0.0008, 0.0028, 0.0036,  ..., 0.0037, 0.0037, 0.0008],\n",
       "         ...,\n",
       "         [0.0009, 0.0031, 0.0033,  ..., 0.0031, 0.0038, 0.0008],\n",
       "         [0.0006, 0.0022, 0.0032,  ..., 0.0032, 0.0034, 0.0007],\n",
       "         [0.0008, 0.0024, 0.0031,  ..., 0.0035, 0.0035, 0.0006]],\n",
       "\n",
       "        [[0.0012, 0.0013, 0.0024,  ..., 0.0013, 0.0017, 0.0010],\n",
       "         [0.0009, 0.0012, 0.0032,  ..., 0.0013, 0.0017, 0.0008],\n",
       "         [0.0011, 0.0019, 0.0016,  ..., 0.0014, 0.0014, 0.0010],\n",
       "         ...,\n",
       "         [0.0007, 0.0018, 0.0016,  ..., 0.0019, 0.0017, 0.0009],\n",
       "         [0.0008, 0.0015, 0.0015,  ..., 0.0017, 0.0018, 0.0008],\n",
       "         [0.0010, 0.0012, 0.0010,  ..., 0.0026, 0.0014, 0.0007]],\n",
       "\n",
       "        [[0.0008, 0.0037, 0.0023,  ..., 0.0016, 0.0018, 0.0016],\n",
       "         [0.0010, 0.0036, 0.0025,  ..., 0.0014, 0.0020, 0.0012],\n",
       "         [0.0011, 0.0032, 0.0018,  ..., 0.0016, 0.0016, 0.0015],\n",
       "         ...,\n",
       "         [0.0006, 0.0023, 0.0018,  ..., 0.0014, 0.0018, 0.0010],\n",
       "         [0.0007, 0.0033, 0.0020,  ..., 0.0014, 0.0016, 0.0014],\n",
       "         [0.0006, 0.0025, 0.0018,  ..., 0.0017, 0.0022, 0.0016]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.0006, 0.0025, 0.0024,  ..., 0.0016, 0.0019, 0.0010],\n",
       "         [0.0005, 0.0031, 0.0026,  ..., 0.0018, 0.0012, 0.0015],\n",
       "         [0.0006, 0.0023, 0.0019,  ..., 0.0019, 0.0011, 0.0013],\n",
       "         ...,\n",
       "         [0.0006, 0.0017, 0.0025,  ..., 0.0027, 0.0010, 0.0009],\n",
       "         [0.0004, 0.0015, 0.0015,  ..., 0.0029, 0.0008, 0.0008],\n",
       "         [0.0004, 0.0019, 0.0023,  ..., 0.0038, 0.0008, 0.0007]],\n",
       "\n",
       "        [[0.0006, 0.0019, 0.0018,  ..., 0.0023, 0.0026, 0.0008],\n",
       "         [0.0006, 0.0014, 0.0020,  ..., 0.0021, 0.0023, 0.0008],\n",
       "         [0.0005, 0.0016, 0.0017,  ..., 0.0023, 0.0018, 0.0006],\n",
       "         ...,\n",
       "         [0.0004, 0.0014, 0.0017,  ..., 0.0048, 0.0014, 0.0004],\n",
       "         [0.0004, 0.0019, 0.0019,  ..., 0.0030, 0.0014, 0.0007],\n",
       "         [0.0003, 0.0016, 0.0019,  ..., 0.0026, 0.0014, 0.0006]],\n",
       "\n",
       "        [[0.0009, 0.0016, 0.0015,  ..., 0.0018, 0.0013, 0.0011],\n",
       "         [0.0007, 0.0014, 0.0021,  ..., 0.0011, 0.0015, 0.0012],\n",
       "         [0.0007, 0.0011, 0.0023,  ..., 0.0021, 0.0015, 0.0009],\n",
       "         ...,\n",
       "         [0.0006, 0.0011, 0.0014,  ..., 0.0028, 0.0014, 0.0006],\n",
       "         [0.0005, 0.0014, 0.0015,  ..., 0.0012, 0.0011, 0.0007],\n",
       "         [0.0006, 0.0011, 0.0014,  ..., 0.0022, 0.0011, 0.0008]]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SelfTransformer(nn.Module):\n",
    "    def __init__(self,src_vocab_size,tgt_vocab_size,d_model,d_ff,n_head,num_encode_layers,num_decode_layers,dropout):\n",
    "        super().__init__()\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size,d_model)\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size,d_model)\n",
    "\n",
    "        self.pe = SelfPositionalEncoding(d_model)\n",
    "\n",
    "        self.encoder = nn.ModuleList()\n",
    "        for _ in range(num_encode_layers):\n",
    "            self.encoder.append(SelfEncoderLayer(d_model,d_ff,n_head,dropout))\n",
    "        self.decoder = nn.ModuleList()\n",
    "        for _ in range(num_decode_layers):\n",
    "            self.decoder.append(SelfDecoderLayer(d_model,d_ff,n_head,dropout))\n",
    "        \n",
    "        self.linear = nn.Linear(d_model,tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self,src,tgt,src_mask = None,tgt_mask = None):\n",
    "        src_embedding = self.src_embedding(src)\n",
    "        src_embedding_pe = self.pe(src_embedding)\n",
    "\n",
    "        tgt_embedding = self.tgt_embedding(tgt)\n",
    "        tgt_embedding_pe = self.pe(tgt_embedding)\n",
    "\n",
    "        encoder_out = src_embedding_pe\n",
    "        for encode_layer in self.encoder:\n",
    "            encoder_out = encode_layer(encoder_out)\n",
    "\n",
    "        decode_out = tgt_embedding_pe\n",
    "        for decoder_layer in self.decoder:\n",
    "            decode_out = decoder_layer(encoder_out,decode_out,src_mask,tgt_mask)\n",
    "\n",
    "\n",
    "        out = torch.softmax(self.linear(decode_out),dim = -1)\n",
    "        return out\n",
    "    \n",
    "\n",
    "src_vocab_size = 500\n",
    "tgt_vocab_size = 500\n",
    "d_model = 200\n",
    "d_ff = 250 \n",
    "n_head = 4 \n",
    "num_encode_layers = 6\n",
    "num_decode_layers = 6\n",
    "dropout = 0.1\n",
    "\n",
    "batch_size = 50\n",
    "seq_len = 10\n",
    "src = torch.randint(0,src_vocab_size,(batch_size,seq_len))\n",
    "tgt = torch.randint(0,src_vocab_size,(batch_size,seq_len))\n",
    "net = SelfTransformer(src_vocab_size,tgt_vocab_size,d_model,d_ff,n_head,num_encode_layers,num_decode_layers,dropout)\n",
    "\n",
    "tgt_mask = torch.triu(torch.ones(seq_len,seq_len),diagonal=1)\n",
    "\n",
    "net(src,tgt,tgt_mask= tgt_mask)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatTTS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
